import re
import scipy.stats as stats
import numpy as np
import pandas as pd
import nibabel as nib
import xml.etree.ElementTree as ET
from sklearn.cluster import DBSCAN
from xml.sax.saxutils import unescape
import json
from .sitk import nib2sitk, sitk2nib

class AfniIO:
    """ This class handles metadata of AFNI integrated nifti file including,
        1. statistic profile generated by AFNI clustsim
    """
    def __init__(self, file_path: str):
        """
        Args:
            file_path:
        """
        # private
        self._thresholds        = dict()
        self._scrubbed_xml      = []

        # public
        self.nii                = nib.load(file_path)
        self.scene_data         = dict()
        self.dataset_rank       = dict()
        self.brick_statsymbol   = dict()
        self.brick_label        = dict()
        self.clustsim_table     = dict()
        self.clustsim_ref       = dict()
        self.cluster_masks      = dict()
        self.xml_unescape_table = {
            "&amp;": "&",
            "&quot;": '"',
            "&apos;": "'",
            "&gt;": ">",
            "&lt;": "<",
            "&#x0a;": ""
        }
        self.view_type = {0: '+orig', 1: '+acpc', 2: '+tlrc'}
        self.anat_type = {0: 'ANAT_SPGR',
                          1: 'ANAT_FSE',
                          2: 'ANAT_EPI',
                          3: 'ANAT_MRAN',
                          4: 'ANAT_CT',
                          5: 'ANAT_SPECT',
                          6: 'ANAT_PET',
                          7: 'ANAT_MRA',
                          8: 'ANAT_BMAP',
                          9: 'ANAT_DIFF',
                          10: 'ANAT_OMRI',
                          11: 'ANAT_BUCK'
                          }
        self.func_type = {0: 'FUNC_FIM',    # 1 value
                          1: 'FUNC_THR',    # obsolete
                          2: 'FUNC_COR',    # fico: correlation
                          3: 'FUNC_TT',     # fitt: t-statistic
                          4: 'FUNC_FT',     # fift: F-statistic
                          5: 'FUNC_ZT',     # fizt: z-score
                          6: 'FUNC_CT',     # fict: Chi squared
                          7: 'FUNC_BT',     # fibt: Beta stat
                          8: 'FUNC_BN',     # fibn: Binomial
                          9: 'FUNC_GT',     # figt: Gamma
                          10: 'FUNC_PT',    # fipt: Poisson
                          11: 'FUNC_BUCK'   # fbuc: bucket
                          }

        self.type_string = {0: "3DIM_HEAD_ANAT",
                            1: "3DIM_HEAD_FUNC",
                            2: "3DIM_GEN_ANAT",
                            3: "3DIM_GEN_FUNC"
                            }

        self._scrubbing()
        self._parse()

    def _scrubbing(self):
        for ext in self.nii.header.extensions:
            if ext.get_code() == 4:  # AFNI extension in NifTI-1.1
                ext_str = unescape(ext.get_content().decode('UTF-8'), self.xml_unescape_table)
                # regex patterns
                pattern_base_tag        = r"<(?P<tag>[^<>]+)>(?P<contents>[^<>]+)"
                pattern_gbg_quote       = r'\s*\"\s*'
                pattern_gbg_quotes      = r'\"\s+\"'
                pattern_contents        = r'\"(?P<contents>[^\"]+)\"'
                pattern_invalid_open    = r"(?P<num_tag>\d[^<>]*)"
                pattern_invalid_close   = r"/(?P<num_tag>\d[^<>]*)"
                pattern_empty_str       = r''

                for tag, contents in re.findall(pattern_base_tag, ' '.join(ext_str.split('\n'))):
                    scrubbed_contents = re.sub(pattern_gbg_quotes, pattern_empty_str, contents)
                    tag = re.sub(pattern_gbg_quotes, pattern_empty_str, tag)
                    matched = re.search(pattern_contents, scrubbed_contents)

                    if re.match(pattern_invalid_open, tag) or re.match(pattern_invalid_close, tag):
                        tag = f'({tag})'
                    else:
                        tag = f'<{tag}>'
                    if matched:
                        scrubbed_contents = matched.group('contents')

                    if scrubbed_contents.isspace():
                        self._scrubbed_xml.append(tag)
                    elif re.match(pattern_gbg_quote, scrubbed_contents):
                        self._scrubbed_xml.append(tag)
                    else:
                        self._scrubbed_xml.append(tag)
                        self._scrubbed_xml.append(f'{scrubbed_contents}')

    def _parse(self):
        # pattern to _parse attributes and contents in clustsim_tag
        pattern_clustsim_tag    = r'\(3dClustSim_\w{2}\d{1}\s+(?P<attributes>[^)]+)\)' \
                                  r'(?P<contents>[^(]*)\(/3dClustSim_\w{2}\d{1}\)'
        # pattern to _parse clustsim condition
        pattern_test_type       = r'^AFNI_CLUSTSIM_(?P<nn>NN\d{1})_(?P<side>\w+sided)$'
        # pattern to _parse dictionary stricture
        pattern_attribute       = r'((?!=")[a-z_]*)\=\"(.+?)\"'

        root = ET.fromstring('\n'.join(self._scrubbed_xml))
        if root.tag == 'AFNI_attributes':
            for child in root:
                if re.match('^SCENE_DATA$', child.attrib['atr_name']):
                    # _parse SCENE_DATA
                    scene_data = np.asarray(child.text.strip().split(), dtype=int)[:3]
                    v, tp, ts = scene_data
                    self.scene_data['view_type'] = self.view_type[v]
                    if 'ANAT' in self.type_string[ts]:
                        self.scene_data['data_type'] = self.anat_type[tp]
                    else:
                        self.scene_data['data_type'] = self.func_type[tp]

                elif re.match('^DATASET_RANK$', child.attrib['atr_name']):
                    # _parse DATASET_RANK
                    dataset_rank = np.asarray(child.text.strip().split(), dtype=int)[:2]
                    self.dataset_rank['spatial_dim'] = dataset_rank[0]
                    self.dataset_rank['num_sub-bricks'] = dataset_rank[1]

                elif re.match('^BRICK_STATSYM', child.attrib['atr_name']):
                    p4 = r'^(?P<test>[a-zA-Z]+)\((?P<param>.*)\)'
                    # parse BRICK_STATSYM - contains info which statistics uses for each sub-bricks
                    stats_type = [t.strip() for t in child.text.split(';')]

                    for i, tp in enumerate(stats_type):
                        if re.match(p4, tp):
                            test_type = re.sub(p4, r'\g<test>', tp)
                            param = re.sub(p4, r'\g<param>', tp)
                            if len(param.split(',')) > 1:
                                param = list(map(int, param.split(',')))
                        else:
                            test_type = None if tp == 'none' else tp
                            param = None

                        self.brick_statsymbol[i] = None if test_type is None and param is None else [test_type, param]

                elif re.match('^BRICK_LAB', child.attrib['atr_name']):
                    # parse BRICK_LAB
                    brick_lab = child.text.strip().strip('"')
                    self.brick_label = {i: t for i, t in enumerate(brick_lab.split('~'))}

                elif re.match(pattern_test_type, child.attrib['atr_name']):
                    # parse clustsim tables
                    clustsim = re.sub(pattern_test_type, r'\g<nn>-\g<side>', child.attrib['atr_name'])
                    text = ''.join(child.text.strip('\n'))
                    if re.match(pattern_clustsim_tag, text):
                        cont = ''.join(re.sub(pattern_clustsim_tag, r'\g<contents>', text)).strip().split(' ')
                        if 'p' not in self.clustsim_ref.keys():
                            attr = re.sub(pattern_clustsim_tag, r'\g<attributes>', text)
                            attr = dict(re.findall(pattern_attribute, attr))
                            p = list(map(float, attr['pthr'].split(',')))
                            self.clustsim_ref['p'] = p
                            a = list(map(float, attr['athr'].split(',')))
                            self.clustsim_ref['a'] = a
                        csim_table = np.asarray(cont, dtype='float').reshape([len(self.clustsim_ref['p']),
                                                                              len(self.clustsim_ref['a'])])
                        self.clustsim_table[clustsim] = pd.DataFrame(csim_table,
                                                                     index=self.clustsim_ref['p'],
                                                                     columns=self.clustsim_ref['a'])

    @property
    def avail_tables(self):
        tables = sorted(self.clustsim_table.keys())
        return {i: tbl for i, tbl in enumerate(tables)}

    def get_cluster_thr(self, table_idx, thr_p, thr_a):
        ref_a = self.clustsim_ref['a']
        xp = self.clustsim_ref['p']
        clustsim_table = self.clustsim_table[self.avail_tables[table_idx]]
        if thr_a not in ref_a:
            raise Exception('Choose a value from: {}'.format(ref_a))
        else:
            yp = clustsim_table[thr_a]
            if not np.all(np.diff(xp) > 0):
                xp = xp[::-1]
                yp = yp[::-1]
            cluster_size = np.interp(thr_p, xp, yp)
        return np.ceil(cluster_size).astype(int)

    @property
    def thresholds(self):
        if self._thresholds:
            return self._thresholds
        else:
            raise Exception('set first!')

    def set_thr(self, nn_level, p_value, alpha):
        self.cluster_masks = dict()     # clear existing mask
        self._thresholds = dict(nn_level    = nn_level,
                                p_value     = p_value,
                                alpha       = alpha)
        self.est_cluster_mask(*self._thresholds)

    def est_cluster_mask(self, nn_level, p_value, alpha):
        img_data = np.asarray(self.nii.dataobj)
        estimator = DBSCAN(eps=np.sqrt(nn_level), min_samples=1)
        for i, test_params in [(k, v) for k, v in self.brick_statsymbol.items() if v is not None]:
            idx_count = 0
            test_type, params = test_params

            if test_type == 'Ftest':
                table_type = 'NN{}-2sided'.format(nn_level)
            elif test_type in ['Ttest', 'Zscore']:
                table_type = 'NN{}-bisided'.format(nn_level)
            else:
                raise Exception('Not supported Stats, contact developer.')

            if table_type not in self.avail_tables.values():
                raise Exception('There is no ClusterSim Table for {}'.format(table_type))
            table_idx = [j for j, ttype in self.avail_tables.items() if ttype == table_type][0]
            cluster_size_thr = self.get_cluster_thr(table_idx, p_value, alpha)

            target_data = img_data[:, :, :, 0, i]
            output_data = np.zeros(target_data.shape)

            if test_type in ['Ttest', 'Zscore']:
                if test_type == 'Ttest':
                    low_t = stats.t.ppf(p_value / 2, params)
                    hgh_t = stats.t.ppf(1 - (p_value / 2), params)
                else:   # Zscore
                    low_t = stats.norm.ppf(p_value / 2)
                    hgh_t = stats.norm.ppf(1 - (p_value / 2))

                low_x_indices = np.transpose(np.nonzero(target_data <= low_t))
                hgh_x_indices = np.transpose(np.nonzero(target_data >= hgh_t))

                low_clusters = estimator.fit(low_x_indices).labels_
                hgh_clusters = estimator.fit(hgh_x_indices).labels_

                output_data, idx_count = self._apply_cluster_size_thr(low_x_indices, output_data, low_clusters,
                                                                      cluster_size_thr, idx_count)
                output_data, idx_count = self._apply_cluster_size_thr(hgh_x_indices, output_data, hgh_clusters,
                                                                      cluster_size_thr, idx_count)

            elif test_type == 'Ftest':
                hgh_t = stats.f.ppf(1 - p_value, *params)
                hgh_x_indices = np.transpose(np.nonzero(target_data >= hgh_t))

                hgh_clusters = estimator.fit(hgh_x_indices).labels_
                output_data, idx_count = self._apply_cluster_size_thr(hgh_x_indices, output_data, hgh_clusters,
                                                                      cluster_size_thr, idx_count)
            else:
                raise Exception('Not supported Stats, contact developer.')

            if idx_count >= 1:
                self.cluster_masks[self.brick_label[i]] = nib.Nifti1Image(output_data, self.nii.affine)

    @staticmethod
    def _apply_cluster_size_thr(x_indices, output_data, clusters, cluster_size, idx_count):
        """ apply cluster size threshold """
        survived_clusters = []
        for c_idx in [c_idx for c_idx in set(clusters) if c_idx != -1]:
            if len(clusters[clusters == c_idx]) >= cluster_size:
                survived_clusters.append(c_idx)

        if len(survived_clusters) != 0:
            for c_idx in survived_clusters:
                idx_count += 1
                for x, y, z in x_indices[np.where(clusters == c_idx)]:
                    output_data[x, y, z] = idx_count
        return output_data, idx_count

def load(file_path: str):
    """
    load available file
    available exts: .nii(.gz), .xls(x), .csv, .tsv, .json

    :param file_path: file want to load
    :type file_path: str
    :return: object
    """
    if file_path.endswith('.nii') or file_path.endswith('.nii.gz'):
        img = nib.Nifti1Image.load(file_path)
    else:
        if file_path.endswith('.xls'):
            img = pd.read_excel(file_path)
        elif file_path.endswith('.csv'):
            img = pd.read_csv(file_path)
        elif file_path.endswith('.tsv'):
            img = pd.read_table(file_path)
        elif file_path.endswith('.1D'):
            img = pd.read_csv(file_path, header=None, sep=r'\s+')
        elif file_path.endswith('.json'):
            img = json.load(open(file_path))
        else:
            raise Exception('Input filetype is not compatible.')
    return img


def load_volreg(path, mean_radius=9):
    """ Return motion parameter estimated from AFNI's 3dvolreg
    radian values will converted to distance based on given radius.

    :param path:        filepath of 1D data
    :param mean_radius: the distance from aural to central fissure of animal (default: 9mm for rat)
    :return:
    """

    def convert_radian2distance(volreg_, mean_radius_):
        volreg_[['Roll', 'Pitch', 'Yaw']] *= (np.pi / 180 * mean_radius_)
        return volreg_

    volreg = load(path)
    volreg.columns = ['Roll', 'Pitch', 'Yaw', 'dI-S', 'dR-L', 'dA-P']
    r = np.round(np.sqrt(2) * mean_radius)
    return convert_radian2distance(volreg, r)


if __name__ == '__main__':
    path = 'examples/afni_3dttest_output.nii.gz'
    mvm = AfniIO(path)
    cluster_masks = mvm.est_cluster_mask(nn_level=2, p_value=0.05, alpha=0.05)
    print(mvm.clustsim_table)
    print(mvm.scene_data)
    print(mvm.dataset_rank)
    # TODO: need to make a table shows alpha for each cluster
